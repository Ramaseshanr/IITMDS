{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0988c3a-1734-4566-8293-7d138aacdf6c",
   "metadata": {},
   "source": [
    "# Sample code for the workshop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef36017-bbd9-4c82-9533-a815bfc38872",
   "metadata": {},
   "source": [
    "## Reading/downloading text available in various formats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c5f951-8c4b-4639-96ae-1cf254ac7e82",
   "metadata": {},
   "source": [
    "# Wiki dumps for various languages\n",
    "\n",
    "## Location of the wiki dump\n",
    "\n",
    "Bengali -  https://dumps.wikimedia.org/bnwiki/latest/bnwiki-latest-pages-articles.xml.bz2\n",
    "\n",
    "Hindi - https://dumps.wikimedia.org/hiwiki/latest/hiwiki-latest-pages-articles.xml.bz2\n",
    "\n",
    "Kannada - https://dumps.wikimedia.org/knwiki/latest/knwiki-latest-pages-articles.xml.bz2\n",
    "\n",
    "Malayalam - https://dumps.wikimedia.org/mlwiki/latest/mlwiki-latest-pages-articles.xml.bz2\n",
    "\n",
    "Marati - https://dumps.wikimedia.org/mrwiki/latest/mrwiki-latest-pages-articles.xml.bz2\n",
    "\n",
    "Tamil - https://dumps.wikimedia.org/tawiki/latest/tawiki-latest-pages-articles.xml.bz2\n",
    "\n",
    "Telugu - https://dumps.wikimedia.org/tewiki/latest/tewiki-latest-pages-articles.xml.bz2\n",
    "\n",
    "For any other language, replace the first two characters of the file name with the ISO code of that language\n",
    "\n",
    "### Use a bz2 extractor to extract XML dump of the articles "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbb8e9f-04bb-4580-b0c9-31c12cc11301",
   "metadata": {},
   "source": [
    "# use the following sample code to extract the dump\n",
    "\n",
    "Note: Intially try with 100 articles from the dump before extracting the entire XML data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d6e5590-4f6d-409f-8031-8642eac56998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: wget in c:\\users\\hayagreev\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.2)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "# Helper code\n",
    "%pip install wget\n",
    "from pathlib import Path\n",
    "import wget\n",
    "import bz2\n",
    "\n",
    "\n",
    "def get_corpus_folder():\n",
    "    home = get_home_dir()\n",
    "    return  home + '/Teaching/IITMDS/Corpora/'\n",
    "\n",
    "def get_home_dir():\n",
    "    return str(Path.home())    \n",
    "\n",
    "\n",
    "def download_wiki(language_code, remote_url):\n",
    "    print('downloading...')\n",
    "    local_file = get_corpus_folder() + language_code + 'Wiki.bz2'\n",
    "    wget.download(remote_url, local_file)\n",
    "    print('Download Complete')\n",
    "    return local_file\n",
    "\n",
    "def download_file(language_code, remote_url):\n",
    "    print('Download started')\n",
    "    req = requests.get(remote_url)\n",
    " \n",
    "    # Split URL to get the file name\n",
    "    filename = remote_url.split('/')[-1]\n",
    "    local_file = get_corpus_folder() + filename\n",
    "\n",
    "    # Writing the file to the local file system\n",
    "    with open(local_file,'wb') as output_file:\n",
    "        output_file.write(req.content)\n",
    "        \n",
    "    print('Download Completed')\n",
    "    print(local_file)\n",
    "    return local_file\n",
    "    \n",
    "def uncompress_bz2(language_code, bz2_filename):\n",
    "    print('Uncompress started')\n",
    "    corpus_filename = get_corpus_folder() + language_code + 'Wiki.txt'\n",
    "    with bz2.open(bz2_filename, \"rb\") as f:\n",
    "        # Decompress data from file\n",
    "        content = f.read()\n",
    "        output = open(corpus_filename, 'w', encoding='utf-8')\n",
    "        output.write(content.decode(encoding='utf-8'))\n",
    "        f.close()\n",
    "    print('Completed')\n",
    "    return corpus_filename    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5810906f-261f-4af6-b0b1-f342a458a58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install wiki_dump_reader\n",
    "\n",
    "from wiki_dump_reader import Cleaner, iterate\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "#https://github.com/CyberZHG/wiki-dump-reader\n",
    "#pip install wiki-dump-reader\n",
    "#Code adapted from https://github.com/CyberZHG/wiki-dump-reader\n",
    "def create_corpus_for_language(language_code):\n",
    "\n",
    "    # corpus_file = '<your_Folder/CorpusFileName.txt'\n",
    "    corpus_filename = get_corpus_folder() + language_code + 'Wiki.txt'\n",
    "    page_count = 0\n",
    "    cleaner = Cleaner()\n",
    "    with open(corpus_filename, 'w', encoding='utf-8') as output:\n",
    "       # for title, text in iterate('/LocationOfYourContent/XXwiki-latest-pages-articles.xml'):\n",
    "        for title, text in iterate(get_home_dir() + '/Downloads/' + language_code + 'wiki-latest-pages-articles.xml'):\n",
    "            text = cleaner.clean_text(text)\n",
    "            cleaned_text, links = cleaner.build_links(text)\n",
    "            output.write(title + '\\n' + cleaned_text + '\\n')\n",
    "            page_count += 1\n",
    "            if page_count % 50 == 0:\n",
    "                print('Pages dumped = ', page_count)\n",
    "            # For demo purposes, the execution is stopped after 5 pages\n",
    "            if page_count > 100:\n",
    "                break\n",
    "    output.close()\n",
    "\n",
    "\n",
    "language_code = 'bn'\n",
    "#local_bz2_file = download_file('bn',\"https://dumps.wikimedia.org/bnwiki/latest/bnwiki-latest-pages-articles.xml.bz2\")\n",
    "#uncompress_bz2(language_code, local_bz2_file)\n",
    "#create_corpus_for_language(language_code)\n",
    "file = open(get_corpus_folder() + language_code + 'Wiki.txt','r', encoding='utf-8')\n",
    "#file = open(get_corpus_folder() + 'sample.txt','r', encoding='utf-8')\n",
    "file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2a54ea-f6cc-445d-bdbb-c4d63643d407",
   "metadata": {},
   "source": [
    "## Exrtaction of text from a json corpus\n",
    "### Corpus compiled in JSON format\n",
    "### JSON schema of full text documents of COVID Corpus\n",
    "### This JSON formated text is extracted from PDF files"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a03fcae5-0a75-4503-844e-a8f9f723ee3f",
   "metadata": {},
   "source": [
    "\n",
    "{\n",
    "    \"paper_id\": <str>,                      # 40-character sha1 of the PDF\n",
    "    \"metadata\": {\n",
    "        \"title\": <str>,\n",
    "        \"authors\": [                        # list of author dicts, in order\n",
    "            {\n",
    "                \"first\": <str>,\n",
    "                \"middle\": <list of str>,\n",
    "                \"last\": <str>,\n",
    "                \"suffix\": <str>,\n",
    "                \"affiliation\": <dict>,\n",
    "                \"email\": <str>\n",
    "            },\n",
    "            ...\n",
    "        ],\n",
    "        \"abstract\": [                       # list of paragraphs in the abstract\n",
    "            {\n",
    "                \"text\": <str>,\n",
    "                \"cite_spans\": [             # list of character indices of inline citations\n",
    "                                            # e.g. citation \"[7]\" occurs at positions 151-154 in \"text\"\n",
    "                                            #      linked to bibliography entry BIBREF3\n",
    "                    {\n",
    "                        \"start\": 151,\n",
    "                        \"end\": 154,\n",
    "                        \"text\": \"[7]\",\n",
    "                        \"ref_id\": \"BIBREF3\"\n",
    "                    },\n",
    "                    ...\n",
    "                ],\n",
    "                \"ref_spans\": <list of dicts similar to cite_spans>,     # e.g. inline reference to \"Table 1\"\n",
    "                \"section\": \"Abstract\"\n",
    "            },\n",
    "            ...\n",
    "        ],\n",
    "        \"body_text\": [                      # list of paragraphs in full body\n",
    "                                            # paragraph dicts look the same as above\n",
    "            {\n",
    "                \"text\": <str>,\n",
    "                \"cite_spans\": [],\n",
    "                \"ref_spans\": [],\n",
    "                \"eq_spans\": [],\n",
    "                \"section\": \"Introduction\"\n",
    "            },\n",
    "            ...\n",
    "            {\n",
    "                ...,\n",
    "                \"section\": \"Conclusion\"\n",
    "            }\n",
    "        ],\n",
    "        \"bib_entries\": {\n",
    "            \"BIBREF0\": {\n",
    "                \"ref_id\": <str>,\n",
    "                \"title\": <str>,\n",
    "                \"authors\": <list of dict>       # same structure as earlier,\n",
    "                                                # but without `affiliation` or `email`\n",
    "                \"year\": <int>,\n",
    "                \"venue\": <str>,\n",
    "                \"volume\": <str>,\n",
    "                \"issn\": <str>,\n",
    "                \"pages\": <str>,\n",
    "                \"other_ids\": {\n",
    "                    \"DOI\": [\n",
    "                        <str>\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            \"BIBREF1\": {},\n",
    "            ...\n",
    "            \"BIBREF25\": {}\n",
    "        },\n",
    "        \"ref_entries\":\n",
    "            \"FIGREF0\": {\n",
    "                \"text\": <str>,                  # figure caption text\n",
    "                \"type\": \"figure\"\n",
    "            },\n",
    "            ...\n",
    "            \"TABREF13\": {\n",
    "                \"text\": <str>,                  # table caption text\n",
    "                \"type\": \"table\"\n",
    "            }\n",
    "        },\n",
    "        \"back_matter\": <list of dict>           # same structure as body_text\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2666e66e-2bc3-424c-9970-bd542896417c",
   "metadata": {},
   "source": [
    "### Code for extracting text from the JSON \n",
    "### the schema is given above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581e17da-c0dc-41e1-8ffb-4031247dd060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def extract_text_from_json_file(filename):\n",
    "    '''\n",
    "    Extract the text from the file name (json file) and\n",
    "    index the content from paper_id, title, abstract and body_text fields\n",
    "    Retuns - text of title, abstract and bodt_text\n",
    "    '''\n",
    "\n",
    "    file = open(filename)\n",
    "    body_text = \"\"\n",
    "    abstract = \"\"\n",
    "    title = \"\"\n",
    "    paper_id = \"\"\n",
    "\n",
    "    paper_content = json.load(file)\n",
    "\n",
    "    #get the paper_id\n",
    "    if 'paper_id' in paper_content:\n",
    "        paper_id = paper_content['paper_id']\n",
    "        \n",
    "    #get the title, if available\n",
    "    if 'title' in paper_content['metadata']:\n",
    "        title = paper_content['metadata']['title']\n",
    "    #get abstract.text, if availabledef extract_text(filename):\n",
    "    '''\n",
    "    Extract the text from the file name (json file) and\n",
    "    index the content from paper_id, title, abstract and body_text fields\n",
    "    Retuns - text of title, abstract and bodt_text\n",
    "    '''\n",
    "\n",
    "    file = open(filename)\n",
    "    body_text = \"\"\n",
    "    abstract = \"\"\n",
    "    title = \"\"\n",
    "    paper_id = \"\"\n",
    "\n",
    "    paper_content = json.load(file)\n",
    "\n",
    "    #get the paper_id\n",
    "    if 'paper_id' in paper_content:\n",
    "        paper_id = paper_content['paper_id']\n",
    "        \n",
    "    #get the title, if available\n",
    "    if 'title' in paper_content['metadata']:\n",
    "        title = paper_content['metadata']['title']\n",
    "    #get abstract.text, i\n",
    "    if 'abstract' in paper_content:\n",
    "        for abs in paper_content['abstract']:\n",
    "            abstract = abstract + abs['text']\n",
    "    if 'body_text' in paper_content:\n",
    "        for bt in paper_content['body_text']:\n",
    "            body_text = body_text + bt['text']\n",
    "\n",
    "\n",
    "   \n",
    "    return (title + ' ' + abstract + ' ' + body_text + ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f09ab8-85f5-475a-85f7-143525a63084",
   "metadata": {},
   "outputs": [],
   "source": [
    "from base64 import decode\n",
    "\n",
    "\n",
    "txt_dump = extract_text_from_json_file(get_corpus_folder()+'covid_paper_sample.json')\n",
    "\n",
    "f = open(get_corpus_folder() + 'sample.txt', 'w',encoding='utf-8')\n",
    "f.write(txt_dump)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2a76b0-5f17-4ec2-afc1-177d7fac28b3",
   "metadata": {},
   "source": [
    "## Upload/read corpus \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7043085c-fabb-4fd4-8245-2b543b20e741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the corpus\n",
    "\n",
    "fd = open(get_corpus_folder() + 'covid19_partial.txt', encoding='utf-8' )\n",
    "covid19_corpus = fd.read()\n",
    "# Create tokens out of the corpus\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "nltk.download('punkt')\n",
    "\n",
    "#remove numbers\n",
    "pp_text = re.sub(r'\\d+', '', covid19_corpus)\n",
    "\n",
    "#tokenize\n",
    "tokens = nltk.word_tokenize(pp_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3708b262-c2d1-40f0-ac92-488367e7e020",
   "metadata": {},
   "source": [
    "## Counting\n",
    "### Number of Tokens in the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3693cc11-2589-4f0e-82c0-1c4ea56b3b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_tokens = len(tokens)\n",
    "print('Total number of tokens is {}'.format(number_of_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e73aa3-33bf-47fa-9e45-8e398a6c1866",
   "metadata": {},
   "source": [
    "## Vocabulary of the corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e2edb5-7404-4f63-b7c5-235c52dd4b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = list(set(tokens))\n",
    "print('The number of words in the vocabulary is {}'.format(len(vocabulary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7368b34-d8bf-43bc-87b7-46e7d8b25be0",
   "metadata": {},
   "source": [
    "## Frequency of words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f9d113-f948-4600-9198-582082fc867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "token_frequency = Counter()\n",
    "token_frequency.update(Counter(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a545af-0575-4063-bb58-59f0e8a8b453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print top 20 high frequency words\n",
    "token_frequency.most_common()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f944ad86-a9e0-47b4-83b3-6253780cc424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print 20 low frequency words\n",
    "token_frequency.most_common()[-21:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb8a76c-ca90-4c8e-8f8a-8368a0bc668f",
   "metadata": {},
   "source": [
    "__________________________________________________\n",
    "## Heaps Law\n",
    "\n",
    "### The estimated vocabulary size is proportional to the total number of tokens in the corpus\n",
    "$$\\begin{align}\n",
    "M &\\propto T^{\\beta}\\\\ \n",
    "M &= \\kappa T^\\beta\\ \\end{align}$$\n",
    "\n",
    "### We can rewrite the above equation as\n",
    "$$\\begin{align}\n",
    "\\log(M) &= \\log(\\kappa) + \\beta \\times \\log(T) \\end{align}$$\n",
    "\n",
    "### where  is the estimated vocabulary,  is the total number of tokens and  is a constant.  is usually between  and \n",
    "### If you consider two words whose frequency and rank are known. \n",
    "### Is it possible to find the alpha value?\n",
    "__________________________________________________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fac4690-1c83-47bb-b42e-8d57c72afb5a",
   "metadata": {},
   "source": [
    "### Find out vocabulary for every 1000 words\n",
    "### Divide the entire tokens in to a list of 1000 word tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91a3227-58ff-4b10-9047-6b3f6c0b0a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {}\n",
    "token_count = 0\n",
    "vocab_count = 0\n",
    "\n",
    "token_list=[]\n",
    "vocab_list=[]\n",
    "\n",
    "for token in tokens:\n",
    "    if token in vocabulary:\n",
    "        vocabulary[token] += 1\n",
    "        token_count +=1\n",
    "    else:\n",
    "        vocabulary[token] = 1\n",
    "        token_count +=1\n",
    "        vocab_count +=1\n",
    "\n",
    "    if token_count%1000==0:\n",
    "        token_list.append(token_count)\n",
    "        vocab_list.append(vocab_count)\n",
    "\n",
    "print('Token_count:', token_count)\n",
    "print('Vocab_count:', vocab_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8ce091-ae21-40e0-93a1-1ac6b454f54d",
   "metadata": {},
   "source": [
    "__________________________________________________\n",
    "### To find $\\beta$ and $k$, convert the list into log values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdf7951-b56f-4082-a1dd-36f59b18f2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "log_t = np.log(token_list)\n",
    "log_m = np.log(vocab_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613159dd-5a20-4fcf-8ba8-38e0a503334a",
   "metadata": {},
   "source": [
    "__________________________________________________\n",
    "### To demonstrate, we use the first two values for the estimation of $\\beta$ and $k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40d8960-b070-4c24-95a0-d143c338ba24",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = (log_m[1] - log_m[0]) / (log_t[1] - log_t[0])\n",
    "log_k = log_m[1] - b * log_t[1]\n",
    "\n",
    "print(\"b = \", b,\", log(k) = \",log_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae38031f-0862-4993-9368-f1dedc396c87",
   "metadata": {},
   "source": [
    "__________________________________________________\n",
    "### Now, estimate the vocabulary using Heap's law\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633087eb-872d-4a70-83c4-fe1d129e8232",
   "metadata": {},
   "outputs": [],
   "source": [
    "heaps_vocab_estimate = []\n",
    "\n",
    "for i in log_t:\n",
    "    heaps_vocab_estimate.append(log_k + b * i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92706481-9c2d-4e6b-bd16-487819b47858",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib  \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "\n",
    "plt.plot(log_t, log_v, label = \"Actual\")\n",
    "plt.plot(log_t, heaps_vocab_estimate, label = \"Estimated\", linestyle='dashdot')\n",
    "plt.xlabel(\"Log of token count\")\n",
    "plt.ylabel(\"Log of vocabulary count\")\n",
    "plt.title(\"Plot to show actual and estimated vocabulary counts\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59836362-dde2-4930-b7c2-e1817b34a0cf",
   "metadata": {},
   "source": [
    "________________________________________________________\n",
    "### Application of Heaps Law\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b6b2b2-6206-4fb9-8f52-dd7e07fa5c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45be82d3-22b6-458f-b6e7-231da8f7e33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "#get the stop words for English\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "words_bryant = nltk.Text(nltk.corpus.gutenberg.words('bryant-stories.txt'))\n",
    "\n",
    "words_emma = nltk.Text(nltk.corpus.gutenberg.words('austen-emma.txt'))\n",
    "\n",
    "#convert to small letters\n",
    "words_bryant = [word.lower() for word in words_bryant if word.isalpha()]\n",
    "words_emma = [word.lower() for word in words_emma if word.isalpha()]\n",
    "\n",
    "#remove stop words\n",
    "words_bryant = [word.lower() for word in words_bryant if word not in stop_words][:15000]\n",
    "words_emma = [word.lower() for word in words_emma if word not in stop_words][:15000]\n",
    "\n",
    "TTR_bryant = len(set(words_bryant))/len(words_bryant)\n",
    "TTR_emma = len(set(words_emma))/len(words_emma)\n",
    "\n",
    "print('Number of tokens, Vocabulary, Type-token ratio (Bryant stories) = ', len(words_bryant), len(set(words_bryant)), TTR_bryant)\n",
    "print('Number of tokens, Vocabulary, Type-token ratio (Jane Austen Emma) = ', len(words_emma), len(set(words_emma)), TTR_emma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a120db5-5cd3-486a-9594-b70f6fb8a845",
   "metadata": {},
   "source": [
    "\n",
    "### According to Zipf's law, the frequency of the word is inversely proportional to its rank.\n",
    "#### $$\\begin{align}\n",
    "f_w &\\propto \\frac{1}{r^{\\alpha}} \\\\ \n",
    "f_w &= \\kappa \\frac{1}{r^{\\alpha}}\\\\ \n",
    "f_w\\times r^{\\alpha} &= \\kappa \\end{align}$$\n",
    "\n",
    "where $\\kappa$ is a constant,  $f$ is the frequency of a word, and $r$ is its rank.\n",
    "Here $\\alpha$ is usually between 0.9 and 1.0\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a975cf3f-6f72-4858-86e9-c92345c4808e",
   "metadata": {},
   "source": [
    "### Plotting the frequency and rank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981e1e1b-4007-4c0a-9f76-4549f097ae0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "v = {}\n",
    "for key, value in reversed(sorted(token_frequency.items(), key = itemgetter(1))):\n",
    "    v[key] = value\n",
    "    \n",
    "# Plotting Zipf's Law\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "#Getting the values needed to plot\n",
    "#You may use the RHS values in the plotting function directly\n",
    "n = len(v.keys())\n",
    "y = list(v.values())\n",
    "x = list(range(1, n+1))\n",
    " \n",
    "plt.plot(x, y)\n",
    " \n",
    "#choosing log scale\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "\n",
    "# naming the x axis\n",
    "plt.ylabel('log(frequency)')\n",
    "# naming the y axis\n",
    "plt.xlabel('log(rank)')\n",
    " \n",
    "\n",
    "plt.title(\"Zipf's Law: Rank vs Frequency graph\")\n",
    "\n",
    "#If you want to save the plot, use the next instruction\n",
    "#plt.savefig(\"zipfs190522.png\") \n",
    "\n",
    "# function to show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5038c1-ab63-42cd-a91f-01d1c2a3ab69",
   "metadata": {},
   "source": [
    "\n",
    "# Exercise 1\n",
    "How do you estimate the $\\alpha$ value using the drequency and rank?\n",
    "Hint: $k$ is a constant :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19f7869-d727-4950-9dd6-3b5968cb47fb",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "## Plot rank Vs frequency graph for atleast 3 languages, using the wiki content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e9fd02-9497-4c7b-aad4-ce10808fae18",
   "metadata": {},
   "source": [
    "### Term frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b2cba9-d6d4-4e76-bc89-3241b9ba1137",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_tf = {key: value/number_of_tokens for key, value in token_frequency.items()}\n",
    "list(normalized_tf.items())[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e769a7-a43e-4f58-98f0-007261ca1d9e",
   "metadata": {},
   "source": [
    "__________________________________________________\n",
    "# Exercise 3\n",
    "## Select and downdoad wiki dumps for three languages \n",
    "## Find the normalized frequency for words for all languages chosen\n",
    "__________________________________________________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf73b1f2-4843-433a-aca3-525cc9e77048",
   "metadata": {},
   "source": [
    "# Exercise 4\n",
    "## Select and downdoad wiki dumps for three languages \n",
    "## Estmate the vocabulary of the each dump and verify whether they match with the actual vocabulay count using Heap's Law\n",
    "__________________________________________________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea944bd-e51e-4fe4-871d-89c4f3c42af5",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8164d6b0-a7d4-49a7-9634-9803db660cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Viral Gene Compression: Complexity and Verification The smallest known biological organisms are, by far, the viruses. One of the unique adaptations that many viruses have aquired is the compression of the genes in their genomes. In this paper we study a formalized model of gene compression in viruses. Specifically, we define a set of constraints that describe viral gene compression strategies and investigate the properties of these constraints from the point of view of genomes as languages. We pay special attention to the finite case (representing real viral genomes) and describe a metric for measuring the level of compression in a real viral genome. An efficient algorithm for establishing this metric is given along with applications to real genomes including automated classification of viruses and prediction of horizontal gene transfer between host and virus. In contrast to the lengthy, often redundant, genomes of higher organisms, the genomes of viruses are extremely efficient in the encoding of their genes. Where mammalian genomes, for example, possess lengthy introns which code for no genes at all, any given segment of a viral genome may be a coding region for several genes. In addition to prefix and suffix overlap of viral genes, some genes may also be encoded in a retrograde fashion (that is, the gene would be read in a direction opposite to other genes). These systems provide evidence that viruses have evolved a special type of information compression technique. Studying this natural compression system in a rigorous setting could yield insight into the structure of viral genomes and may contribute to a basis for classifying such structures.In this paper, we will specifically consider the types of compression seen in two small double-stranded DNA virus families, Papillomavirus and Polyomavirus, and single-stranded RNA viruses from the Bornavirus, Coronavirus and, to a lesser extent, the Filovirus and Retrovirus families.The importance of this genetic compression becomes obvious when considering the structure of viruses. Viruses generally consist of two principal components: a protein capsid, and genetic material inside the capsid. The capsid serves as protection for the genetic material and also as a mechanism for inserting the genetic material into a host cell. The genetic material may consist of single-or double-stranded DNA or RNA and, in some rare cases, a mixture of the former possibly also including proteins.The need for compression stems from the fact that the size of the capsid limits the amount of room for genetic information inside the virus. In the case of Polyomaviruses, the genome is constrained to be approximately 5kbp (5,000 basepairs) of DNA (compare to the human genome of size 3,150,000 kb), yet still manages to encode 6 distinct genes.We exposit here a formal model of the viral compression techniques in terms of constraints on languages. For example, we would say that a language satisfies the \"viral overlapping compression\" property if some prefix of some word in the language is also a suffix of some other word in the language. We can likewise define constraints for other viral compression techniques, including retrograde encodings. We will focus here on deterministic modeling of the gene-level mechanics, in contrast to the probabilistic analysis of [8] , which addresses gene compression from the point of view of evolutionary pressures and constraints on entire genomes.The organization of the paper is as follows: In section 2 we consider basic notation and prerequisites. In section 3 we define formal versions of the basic viral compression techniques and investigate relationships and dependencies between them. We consider also the question of for which families of languages it is possible to decide these properties. Section 4 focuses on the finitary case of the problem as this is the most interesting from the point of view of applied viral genetics. We present efficient algorithms to decide each of the properties for real viral genomes. Section 5 contains our conclusions and a discussion of practical applications.For a general introduction to virology, we refer the reader to [3] and [10] ; for formal language theory preliminaries, we refer to [9] . Let Σ be a finite alphabet. We denote, by Σ * and Σ + , the sets of words and non-empty words, respectively, over Σ and the empty word by λ. A language L is any subset of Σ * . For a word w ∈ Σ * , we denote the length of w by |w| and the reversal of w by w R . Let N be the set of positive integers. Furthermore, for k ∈ N, define Σ ≥k = {w ∈ Σ * | |w| ≥ k}.A full trio is a language family closed under homomorphism, inverse homomorphism and intersection with regular sets. A full trio is also referred to as a cone. It is known that every full trio is closed under arbitrary a-transductions 1 and hence arbitrary gsm mappings. We refer to [1, 4] for the theory of AFL\\'s.For a binary relation ⊆ Σ * × Σ * and a language L ⊆ Σ * , we defineWe will consider the following well-known relations. Let w, v ∈ Σ * .Also, for each of the relations above, we prepend the word \"proper\", which will be denoted by < p , < s , < i where we enforce that x, y ∈ Σ + above.For example,Before formally stating the definitions of the viral properties, we will define the following sets which will be used for the conditions.Let L ⊆ Σ * be a language, and let n ∈ N such that 1 ≤ n ≤ 6 and let k ∈ N. Then we define the following sets:So, for example, Z(i, L, k) consists of all words w ∈ L such that there exists a word u of length at least k, a non-empty word x and a word v whereby xu is in L and w = uv which is also in L.We now define the properties that we will study.Let L ⊆ Σ * , let n satisfy 1 ≤ n ≤ 6 and let k, l ∈ N. We say that L satisfies condition W (n, k, l) if |Z(n, L, k)| ≥ l.We also call condition W (1, k, l) the l-weak k-prefix overlapping property, condition W (2, k, l) the l-weak k-suffix overlapping property, condition W (3, k, l) the l-weak k-overlapping property, condition W (4, k, l) the l-weak k-double-sided overlapping property, condition W (5, k, l) the l-weak k-retrograde overlapping property and condition W (6, k, l) the l-weak k-concatenated retrograde overlapping property.For example, a language L satisfies W (1, k, l) if and only if there exists l distinct words w ∈ L whereby w = uv for some u, v, x ∈ Σ * , with u of length at least k,x non-empty and xu ∈ L.We also define a strong version of the properties above.We also refer to each of these properties by replacing the prefix \"l-weak\" of each condition above with \"strong\". 2 We now consider the relationships of these properties to each other. The following is immediate from the definitions.Then the following are true:Also, we note the following:Then the following are true:Proof. The first three statements are straightforward. For the fourth statement,Combining Lemma 1, 2, we obtain:Then the following statements are true:We see, however that if L 1 = {abc, aa} andWe also define the following sets which we will use for a characterization.Hence, there exists x ∈ Σ + such that xu ∈ L. Let i = 2. \"⊆\" Let w ∈ U (2, L, k). Thus, there exists v ∈ Σ ≥k , u ∈ Σ * , y ∈ Σ + , vy ∈ L, w = uv. Therefore, v ∈< −1 p (L) ∩ Σ ≥k and w ∈≤ s (< −1 p (L) ∩ Σ ≥k ). \"⊇\" Let w ∈≤ s (< −1 p (L) ∩ Σ ≥k ). Thus, there exists u, v ∈ Σ * such that w = uv with v ∈< −1 p (L) ∩ Σ ≥k . Hence, there exists x ∈ Σ + such that vx ∈ L.Let i = 3. Immediate from case 1, 2. Let i = 4. \"⊆\" Let w ∈ U (4, L, k). Thus, there exists u, v ∈ Σ ≥k , x, y ∈ Σ + , w = uv, (xu ∈ L∧vy ∈ L). Therefore, u ∈< −1Hence, there exists x, y ∈ Σ + such that xu ∈ L and vy ∈ L.Let i = 5. \"⊆\" Let w ∈ U (5, L, k) .This leads naturally to some decision problems. One would like to provide algorithms to test whether languages (or genomes) satisfy these properties. Namely, can we decide whether a given language satisfies one of the properties, depending on the language family that the given language is in? For each weak condition, this amounts to deciding whether |Z(i, L, k)| ≥ l and for each strong condition, it amounts to deciding whether Z(i, L, k) = L. Proposition 3. Let L 1 , L 2 be language families effectively closed under intersection and the full trio operations with L 1 being effectively semilinear and L 2 having a decidable equality problem. Then the following are true:1. For each k, l ∈ N and i, 1 ≤ i ≤ 4, it is decidable whether L ∈ L 1 satisfies W (i, k, l) and it is decidable whether L ∈ L 2 satisfies V (i, k). 2. If L 1 , L 2 are also effectively closed under reversal, then it is decidable whether L ∈ L 1 satisfies W (5, k, l) and it is decidable whether L ∈ L 2 satisfies V (5, k). 3. If L 1 , L 2 are also effectively closed under reversal and +, then it is decidable whether L ∈ L 1 satisfies W (6, k, l) and it is decidable whether L ∈ L 2 satisfies V (6, k).Also, every intersectionclosed full trio is closed under union and concatenation since L 1 $Σ * ∩ Σ * $L 2 is in L 1 and L 2 , there is an a-transducer which outputs L 1 ∪ L 2 and there is a homomorphism which outputs L 1 L 2 . Thus, Z(1, L, k), Z(2, L, k), Z(3, L, k), Z(4, L, k) are in L 1 and L 2 . Additionally, if L 1 , L 2 are closed under reversal, then Z(5, L, k) is in L 1 and L 2 and if L 1 , L 2 are closed under reversal and +, then Z(6, L, k) is in L 1 and L 2 . Since L 1 is effectively semilinear, we can decide if L ∈ L 1 is infinite [5] and if it is not, then we can effectively find the length of the longest word in L. Then, we can test membership of every word of length less than or equal to that length to determine whether |Z(i, L, k)| ≥ l (emptiness is always decidable for semilinear sets, and since L 1 is closed under intersection with regular languages, we can decide whether w ∈ L by testing whether L∩{w} = ∅). Also, by the decidability of equality for L 2 , the proposition follows.We denote by NCM the family of languages defined by one-way nondeterministic, reversal-bounded multicounter machines. It is known that NCM is an intersection and reversal closed full trio effectively closed under semilinearity [7] . Also, it is known that the family of regular languages is closed under all of the operations above and has a decidable equality problem.For each L ∈ NCM, each i, 1 ≤ i ≤ 5 and each k, l ∈ N, it is decidable whether L satisfies W (i, k, l). In addition, for each L ∈ REG, each i, 1 ≤ i ≤ 6 and each k, l ∈ N, it is decidable whether L satisfies W (i, k, l) and V (i, k).Ideally, one would like to apply the formal definitions given here to real viral genomes as a method for classifying viruses based on gene compression. In this section we will consider fast algorithms to do exactly this, and their complexity. Since all real viral genomes are finite, we will restrict ourselves to dealing with finite input languages here. We will describe algorithms which will verify each of the viral properties for a given input viral genome. A viral genome is a finite language in which the words are the genes of the virus.For a finite language L ⊆ Σ + , we let s L be the sum of the lengths of every word of L (the length of the genome).We recall a well-known and important result from [2] . A partial deterministic finite automaton is a deterministic finite automaton in which each state need not have a transition on every letter. The smallest partial DFA for a given regular language is the partial DFA that recognizes the language and has the smallest number of states. In [2] , it is demonstrated that, for each word w ∈ Σ * , the smallest partial DFA accepting For each algorithm in this section, we assume that we have some encoding of L as input, whereby there is only one copy of each word given. We have discussed above how to perform the method suffix dfa. It is easy to pass in the reversal of a language to suffix dfa, in time linear in s L . Then, in line 5 of Algorithm 1, we can check to see if the intersection is empty by keeping a counter starting at k and running w through the transition function of M , decreasing the counter at each step. Then, when the counter reaches 0, we test every state we hit on input w to see whether it is a final state. If it is, we increase l 1 and set v 1 indicating that w ∈ Z(1, L, k). Also, in line 8, we are testing whether w R ∈ Z(1, L R , k). Indeed, by Lemma 2(3), w R ∈ Z(1, L R , k) if and only if w ∈ Z(2, L, k). Thus, if this is true, we increase l 2 and set v 2 to true. In addition, w ∈ Z (3, L, k) if and only if w ∈ Z(1, L, k) ∪ Z(2, L, k) and so we increase l 3 if and only if either v 1 or v 2 is true, and we reset each to false. In this way, when the method completes, l 1 , l 2 and l 3 will be the maximum such that L satisfies W (1, k, l 1 ), W (2, k, l 2 ) and W (3, k, l 3 ), respectively. Furthermore, this method runs in time O(s L ) time. In addition, it is well-known that we can test whether a word w is in the language generated by an NFA in time O(|Q||w|) (see [6] ). Thus, to find the largest integer l 6 such that L satisfies W (6, k, l), we construct the NFA from L and decide membership of w R for each w ∈ L. This takes time O(|w 1 ||Q| + · · · + |w m ||Q|) = O(|Q|s L ). Thus, one can decide whether a finite language L satisfies W (6, k, l) in time O(s 2 L ). Finally, the strong properties can also be verified straightforwardly using the algorithms presented above. Indeed, they are just a special case where l = |L|. We summarize the preceding thusly: Proposition 4. Let i satisfy 1 ≤ i ≤ 5 and let Σ be some fixed alphabet. Then given a finite language L ⊆ Σ + as input without duplicates and k ∈ N, we can both find the largest integer l such that L satisfies W (i, k, l) and we can decide whether L satisfies V (i, k) in time O(s L ). Furthermore, we can both find the largest l whereby L satisfies W (6, k, l) and we can decide whether L satisfies V (6, k) in time O(s 2 L ).We have presented here a formalization of the process of gene compression that occurs in many viral genomes. We have shown dependencies and relationships between these properties and demonstrated that, in general, most of the weak versions of the properties can be decided for languages defined by nondeterministic finite automata augmented with reversal-bounded counters while the strong versions can be decided for regular languages. Most significantly, we have given algorithms which can efficiently decide these properties for real viral genomes and provide information which is immediately useful to virologists. These algorithms give us the ability to study the relative amount of gene compression between related viruses in a quantifiable way. It may be possible to infer evolutionary relationships between viruses using this information. The fact that genes overlap one another provides a very serious constraint for viral genome evolution. It is known that viruses occasionally aquire genes horizontally (that is, a gene from an infected host becomes part of the virus\\'s own genome). Clearly, only those genes which meet very specific constraints (e.g. those that are \"compressible\" relative to the virus\\'s genome) will be able to be incorporated into the virus. Using the algorithms presented here and real viral genome data, we can find target genes in the host organism which, due to their structure, have the greatest probability of being incorporated into the viral genome.Finally, the formal properties here also present a framework for automated classification of a virus given only its genome. The family of Coronaviruses, for example, has a very regular genomic structure: a single strand of +-sense RNA of length 27-30kb. The beginning of this RNA strand always encodes a viral polymerase (often as part of a polyprotein) and the remainder encodes a series of \"nested\" genes. Each of these nested genes is a proper suffix of the previous gene.This structure can obviously be formally encoded using the properties given here. Similar compression regularities can be found in other viral genomes and encoded using our properties. Classification of a new virus is then simply a matter of verifying compliance to our properties and then checking to see if this matches any known structures.By formalizing this ancient form of data compression, we have provided tools which will allow for further insight in the molecular evolution of viruses and assist in the automated classification of new viruses by reference to only their genomes. '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd = open(get_corpus_folder() + 'sample.txt', encoding='utf-8' )\n",
    "sample_txt = fd.read()\n",
    "sample_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e857e067-6733-4917-9df8-cea183d45ae6",
   "metadata": {},
   "source": [
    "__________________________________________________\n",
    "## Case folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b31975a-078c-4485-a0c7-00e97c2858d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'viral gene compression: complexity and verification the smallest known biological organisms are, by far, the viruses. one of the unique adaptations that many viruses have aquired is the compression of the genes in their genomes. in this paper we study a formalized model of gene compression in viruses. specifically, we define a set of constraints that describe viral gene compression strategies and investigate the properties of these constraints from the point of view of genomes as languages. we pay special attention to the finite case (representing real viral genomes) and describe a metric for measuring the level of compression in a real viral genome. an efficient algorithm for establishing this metric is given along with applications to real genomes including automated classification of viruses and prediction of horizontal gene transfer between host and virus. in contrast to the lengthy, often redundant, genomes of higher organisms, the genomes of viruses are extremely efficient in the encoding of their genes. where mammalian genomes, for example, possess lengthy introns which code for no genes at all, any given segment of a viral genome may be a coding region for several genes. in addition to prefix and suffix overlap of viral genes, some genes may also be encoded in a retrograde fashion (that is, the gene would be read in a direction opposite to other genes). these systems provide evidence that viruses have evolved a special type of information compression technique. studying this natural compression system in a rigorous setting could yield insight into the structure of viral genomes and may contribute to a basis for classifying such structures.in this paper, we will specifically consider the types of compression seen in two small double-stranded dna virus families, papillomavirus and polyomavirus, and single-stranded rna viruses from the bornavirus, coronavirus and, to a lesser extent, the filovirus and retrovirus families.the importance of this genetic compression becomes obvious when considering the structure of viruses. viruses generally consist of two principal components: a protein capsid, and genetic material inside the capsid. the capsid serves as protection for the genetic material and also as a mechanism for inserting the genetic material into a host cell. the genetic material may consist of single-or double-stranded dna or rna and, in some rare cases, a mixture of the former possibly also including proteins.the need for compression stems from the fact that the size of the capsid limits the amount of room for genetic information inside the virus. in the case of polyomaviruses, the genome is constrained to be approximately 5kbp (5,000 basepairs) of dna (compare to the human genome of size 3,150,000 kb), yet still manages to encode 6 distinct genes.we exposit here a formal model of the viral compression techniques in terms of constraints on languages. for example, we would say that a language satisfies the \"viral overlapping compression\" property if some prefix of some word in the language is also a suffix of some other word in the language. we can likewise define constraints for other viral compression techniques, including retrograde encodings. we will focus here on deterministic modeling of the gene-level mechanics, in contrast to the probabilistic analysis of [8] , which addresses gene compression from the point of view of evolutionary pressures and constraints on entire genomes.the organization of the paper is as follows: in section 2 we consider basic notation and prerequisites. in section 3 we define formal versions of the basic viral compression techniques and investigate relationships and dependencies between them. we consider also the question of for which families of languages it is possible to decide these properties. section 4 focuses on the finitary case of the problem as this is the most interesting from the point of view of applied viral genetics. we present efficient algorithms to decide each of the properties for real viral genomes. section 5 contains our conclusions and a discussion of practical applications.for a general introduction to virology, we refer the reader to [3] and [10] ; for formal language theory preliminaries, we refer to [9] . let σ be a finite alphabet. we denote, by σ * and σ + , the sets of words and non-empty words, respectively, over σ and the empty word by λ. a language l is any subset of σ * . for a word w ∈ σ * , we denote the length of w by |w| and the reversal of w by w r . let n be the set of positive integers. furthermore, for k ∈ n, define σ ≥k = {w ∈ σ * | |w| ≥ k}.a full trio is a language family closed under homomorphism, inverse homomorphism and intersection with regular sets. a full trio is also referred to as a cone. it is known that every full trio is closed under arbitrary a-transductions 1 and hence arbitrary gsm mappings. we refer to [1, 4] for the theory of afl\\'s.for a binary relation ⊆ σ * × σ * and a language l ⊆ σ * , we definewe will consider the following well-known relations. let w, v ∈ σ * .also, for each of the relations above, we prepend the word \"proper\", which will be denoted by < p , < s , < i where we enforce that x, y ∈ σ + above.for example,before formally stating the definitions of the viral properties, we will define the following sets which will be used for the conditions.let l ⊆ σ * be a language, and let n ∈ n such that 1 ≤ n ≤ 6 and let k ∈ n. then we define the following sets:so, for example, z(i, l, k) consists of all words w ∈ l such that there exists a word u of length at least k, a non-empty word x and a word v whereby xu is in l and w = uv which is also in l.we now define the properties that we will study.let l ⊆ σ * , let n satisfy 1 ≤ n ≤ 6 and let k, l ∈ n. we say that l satisfies condition w (n, k, l) if |z(n, l, k)| ≥ l.we also call condition w (1, k, l) the l-weak k-prefix overlapping property, condition w (2, k, l) the l-weak k-suffix overlapping property, condition w (3, k, l) the l-weak k-overlapping property, condition w (4, k, l) the l-weak k-double-sided overlapping property, condition w (5, k, l) the l-weak k-retrograde overlapping property and condition w (6, k, l) the l-weak k-concatenated retrograde overlapping property.for example, a language l satisfies w (1, k, l) if and only if there exists l distinct words w ∈ l whereby w = uv for some u, v, x ∈ σ * , with u of length at least k,x non-empty and xu ∈ l.we also define a strong version of the properties above.we also refer to each of these properties by replacing the prefix \"l-weak\" of each condition above with \"strong\". 2 we now consider the relationships of these properties to each other. the following is immediate from the definitions.then the following are true:also, we note the following:then the following are true:proof. the first three statements are straightforward. for the fourth statement,combining lemma 1, 2, we obtain:then the following statements are true:we see, however that if l 1 = {abc, aa} andwe also define the following sets which we will use for a characterization.hence, there exists x ∈ σ + such that xu ∈ l. let i = 2. \"⊆\" let w ∈ u (2, l, k). thus, there exists v ∈ σ ≥k , u ∈ σ * , y ∈ σ + , vy ∈ l, w = uv. therefore, v ∈< −1 p (l) ∩ σ ≥k and w ∈≤ s (< −1 p (l) ∩ σ ≥k ). \"⊇\" let w ∈≤ s (< −1 p (l) ∩ σ ≥k ). thus, there exists u, v ∈ σ * such that w = uv with v ∈< −1 p (l) ∩ σ ≥k . hence, there exists x ∈ σ + such that vx ∈ l.let i = 3. immediate from case 1, 2. let i = 4. \"⊆\" let w ∈ u (4, l, k). thus, there exists u, v ∈ σ ≥k , x, y ∈ σ + , w = uv, (xu ∈ l∧vy ∈ l). therefore, u ∈< −1hence, there exists x, y ∈ σ + such that xu ∈ l and vy ∈ l.let i = 5. \"⊆\" let w ∈ u (5, l, k) .this leads naturally to some decision problems. one would like to provide algorithms to test whether languages (or genomes) satisfy these properties. namely, can we decide whether a given language satisfies one of the properties, depending on the language family that the given language is in? for each weak condition, this amounts to deciding whether |z(i, l, k)| ≥ l and for each strong condition, it amounts to deciding whether z(i, l, k) = l. proposition 3. let l 1 , l 2 be language families effectively closed under intersection and the full trio operations with l 1 being effectively semilinear and l 2 having a decidable equality problem. then the following are true:1. for each k, l ∈ n and i, 1 ≤ i ≤ 4, it is decidable whether l ∈ l 1 satisfies w (i, k, l) and it is decidable whether l ∈ l 2 satisfies v (i, k). 2. if l 1 , l 2 are also effectively closed under reversal, then it is decidable whether l ∈ l 1 satisfies w (5, k, l) and it is decidable whether l ∈ l 2 satisfies v (5, k). 3. if l 1 , l 2 are also effectively closed under reversal and +, then it is decidable whether l ∈ l 1 satisfies w (6, k, l) and it is decidable whether l ∈ l 2 satisfies v (6, k).also, every intersectionclosed full trio is closed under union and concatenation since l 1 $σ * ∩ σ * $l 2 is in l 1 and l 2 , there is an a-transducer which outputs l 1 ∪ l 2 and there is a homomorphism which outputs l 1 l 2 . thus, z(1, l, k), z(2, l, k), z(3, l, k), z(4, l, k) are in l 1 and l 2 . additionally, if l 1 , l 2 are closed under reversal, then z(5, l, k) is in l 1 and l 2 and if l 1 , l 2 are closed under reversal and +, then z(6, l, k) is in l 1 and l 2 . since l 1 is effectively semilinear, we can decide if l ∈ l 1 is infinite [5] and if it is not, then we can effectively find the length of the longest word in l. then, we can test membership of every word of length less than or equal to that length to determine whether |z(i, l, k)| ≥ l (emptiness is always decidable for semilinear sets, and since l 1 is closed under intersection with regular languages, we can decide whether w ∈ l by testing whether l∩{w} = ∅). also, by the decidability of equality for l 2 , the proposition follows.we denote by ncm the family of languages defined by one-way nondeterministic, reversal-bounded multicounter machines. it is known that ncm is an intersection and reversal closed full trio effectively closed under semilinearity [7] . also, it is known that the family of regular languages is closed under all of the operations above and has a decidable equality problem.for each l ∈ ncm, each i, 1 ≤ i ≤ 5 and each k, l ∈ n, it is decidable whether l satisfies w (i, k, l). in addition, for each l ∈ reg, each i, 1 ≤ i ≤ 6 and each k, l ∈ n, it is decidable whether l satisfies w (i, k, l) and v (i, k).ideally, one would like to apply the formal definitions given here to real viral genomes as a method for classifying viruses based on gene compression. in this section we will consider fast algorithms to do exactly this, and their complexity. since all real viral genomes are finite, we will restrict ourselves to dealing with finite input languages here. we will describe algorithms which will verify each of the viral properties for a given input viral genome. a viral genome is a finite language in which the words are the genes of the virus.for a finite language l ⊆ σ + , we let s l be the sum of the lengths of every word of l (the length of the genome).we recall a well-known and important result from [2] . a partial deterministic finite automaton is a deterministic finite automaton in which each state need not have a transition on every letter. the smallest partial dfa for a given regular language is the partial dfa that recognizes the language and has the smallest number of states. in [2] , it is demonstrated that, for each word w ∈ σ * , the smallest partial dfa accepting for each algorithm in this section, we assume that we have some encoding of l as input, whereby there is only one copy of each word given. we have discussed above how to perform the method suffix dfa. it is easy to pass in the reversal of a language to suffix dfa, in time linear in s l . then, in line 5 of algorithm 1, we can check to see if the intersection is empty by keeping a counter starting at k and running w through the transition function of m , decreasing the counter at each step. then, when the counter reaches 0, we test every state we hit on input w to see whether it is a final state. if it is, we increase l 1 and set v 1 indicating that w ∈ z(1, l, k). also, in line 8, we are testing whether w r ∈ z(1, l r , k). indeed, by lemma 2(3), w r ∈ z(1, l r , k) if and only if w ∈ z(2, l, k). thus, if this is true, we increase l 2 and set v 2 to true. in addition, w ∈ z (3, l, k) if and only if w ∈ z(1, l, k) ∪ z(2, l, k) and so we increase l 3 if and only if either v 1 or v 2 is true, and we reset each to false. in this way, when the method completes, l 1 , l 2 and l 3 will be the maximum such that l satisfies w (1, k, l 1 ), w (2, k, l 2 ) and w (3, k, l 3 ), respectively. furthermore, this method runs in time o(s l ) time. in addition, it is well-known that we can test whether a word w is in the language generated by an nfa in time o(|q||w|) (see [6] ). thus, to find the largest integer l 6 such that l satisfies w (6, k, l), we construct the nfa from l and decide membership of w r for each w ∈ l. this takes time o(|w 1 ||q| + · · · + |w m ||q|) = o(|q|s l ). thus, one can decide whether a finite language l satisfies w (6, k, l) in time o(s 2 l ). finally, the strong properties can also be verified straightforwardly using the algorithms presented above. indeed, they are just a special case where l = |l|. we summarize the preceding thusly: proposition 4. let i satisfy 1 ≤ i ≤ 5 and let σ be some fixed alphabet. then given a finite language l ⊆ σ + as input without duplicates and k ∈ n, we can both find the largest integer l such that l satisfies w (i, k, l) and we can decide whether l satisfies v (i, k) in time o(s l ). furthermore, we can both find the largest l whereby l satisfies w (6, k, l) and we can decide whether l satisfies v (6, k) in time o(s 2 l ).we have presented here a formalization of the process of gene compression that occurs in many viral genomes. we have shown dependencies and relationships between these properties and demonstrated that, in general, most of the weak versions of the properties can be decided for languages defined by nondeterministic finite automata augmented with reversal-bounded counters while the strong versions can be decided for regular languages. most significantly, we have given algorithms which can efficiently decide these properties for real viral genomes and provide information which is immediately useful to virologists. these algorithms give us the ability to study the relative amount of gene compression between related viruses in a quantifiable way. it may be possible to infer evolutionary relationships between viruses using this information. the fact that genes overlap one another provides a very serious constraint for viral genome evolution. it is known that viruses occasionally aquire genes horizontally (that is, a gene from an infected host becomes part of the virus\\'s own genome). clearly, only those genes which meet very specific constraints (e.g. those that are \"compressible\" relative to the virus\\'s genome) will be able to be incorporated into the virus. using the algorithms presented here and real viral genome data, we can find target genes in the host organism which, due to their structure, have the greatest probability of being incorporated into the viral genome.finally, the formal properties here also present a framework for automated classification of a virus given only its genome. the family of coronaviruses, for example, has a very regular genomic structure: a single strand of +-sense rna of length 27-30kb. the beginning of this rna strand always encodes a viral polymerase (often as part of a polyprotein) and the remainder encodes a series of \"nested\" genes. each of these nested genes is a proper suffix of the previous gene.this structure can obviously be formally encoded using the properties given here. similar compression regularities can be found in other viral genomes and encoded using our properties. classification of a new virus is then simply a matter of verifying compliance to our properties and then checking to see if this matches any known structures.by formalizing this ancient form of data compression, we have provided tools which will allow for further insight in the molecular evolution of viruses and assist in the automated classification of new viruses by reference to only their genomes. '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_txt.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc6fd0f-c56c-4438-bf2a-0c5d7976eb70",
   "metadata": {},
   "source": [
    "____________\n",
    "## Remove numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69dbc117-4e7f-4caf-8e31-d2bb1c9ca03f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Viral Gene Compression: Complexity and Verification The smallest known biological organisms are, by far, the viruses. One of the unique adaptations that many viruses have aquired is the compression of the genes in their genomes. In this paper we study a formalized model of gene compression in viruses. Specifically, we define a set of constraints that describe viral gene compression strategies and investigate the properties of these constraints from the point of view of genomes as languages. We pay special attention to the finite case (representing real viral genomes) and describe a metric for measuring the level of compression in a real viral genome. An efficient algorithm for establishing this metric is given along with applications to real genomes including automated classification of viruses and prediction of horizontal gene transfer between host and virus. In contrast to the lengthy, often redundant, genomes of higher organisms, the genomes of viruses are extremely efficient in the encoding of their genes. Where mammalian genomes, for example, possess lengthy introns which code for no genes at all, any given segment of a viral genome may be a coding region for several genes. In addition to prefix and suffix overlap of viral genes, some genes may also be encoded in a retrograde fashion (that is, the gene would be read in a direction opposite to other genes). These systems provide evidence that viruses have evolved a special type of information compression technique. Studying this natural compression system in a rigorous setting could yield insight into the structure of viral genomes and may contribute to a basis for classifying such structures.In this paper, we will specifically consider the types of compression seen in two small double-stranded DNA virus families, Papillomavirus and Polyomavirus, and single-stranded RNA viruses from the Bornavirus, Coronavirus and, to a lesser extent, the Filovirus and Retrovirus families.The importance of this genetic compression becomes obvious when considering the structure of viruses. Viruses generally consist of two principal components: a protein capsid, and genetic material inside the capsid. The capsid serves as protection for the genetic material and also as a mechanism for inserting the genetic material into a host cell. The genetic material may consist of single-or double-stranded DNA or RNA and, in some rare cases, a mixture of the former possibly also including proteins.The need for compression stems from the fact that the size of the capsid limits the amount of room for genetic information inside the virus. In the case of Polyomaviruses, the genome is constrained to be approximately kbp (, basepairs) of DNA (compare to the human genome of size ,, kb), yet still manages to encode  distinct genes.We exposit here a formal model of the viral compression techniques in terms of constraints on languages. For example, we would say that a language satisfies the \"viral overlapping compression\" property if some prefix of some word in the language is also a suffix of some other word in the language. We can likewise define constraints for other viral compression techniques, including retrograde encodings. We will focus here on deterministic modeling of the gene-level mechanics, in contrast to the probabilistic analysis of [] , which addresses gene compression from the point of view of evolutionary pressures and constraints on entire genomes.The organization of the paper is as follows: In section  we consider basic notation and prerequisites. In section  we define formal versions of the basic viral compression techniques and investigate relationships and dependencies between them. We consider also the question of for which families of languages it is possible to decide these properties. Section  focuses on the finitary case of the problem as this is the most interesting from the point of view of applied viral genetics. We present efficient algorithms to decide each of the properties for real viral genomes. Section  contains our conclusions and a discussion of practical applications.For a general introduction to virology, we refer the reader to [] and [] ; for formal language theory preliminaries, we refer to [] . Let Σ be a finite alphabet. We denote, by Σ * and Σ + , the sets of words and non-empty words, respectively, over Σ and the empty word by λ. A language L is any subset of Σ * . For a word w ∈ Σ * , we denote the length of w by |w| and the reversal of w by w R . Let N be the set of positive integers. Furthermore, for k ∈ N, define Σ ≥k = {w ∈ Σ * | |w| ≥ k}.A full trio is a language family closed under homomorphism, inverse homomorphism and intersection with regular sets. A full trio is also referred to as a cone. It is known that every full trio is closed under arbitrary a-transductions  and hence arbitrary gsm mappings. We refer to [, ] for the theory of AFL\\'s.For a binary relation ⊆ Σ * × Σ * and a language L ⊆ Σ * , we defineWe will consider the following well-known relations. Let w, v ∈ Σ * .Also, for each of the relations above, we prepend the word \"proper\", which will be denoted by < p , < s , < i where we enforce that x, y ∈ Σ + above.For example,Before formally stating the definitions of the viral properties, we will define the following sets which will be used for the conditions.Let L ⊆ Σ * be a language, and let n ∈ N such that  ≤ n ≤  and let k ∈ N. Then we define the following sets:So, for example, Z(i, L, k) consists of all words w ∈ L such that there exists a word u of length at least k, a non-empty word x and a word v whereby xu is in L and w = uv which is also in L.We now define the properties that we will study.Let L ⊆ Σ * , let n satisfy  ≤ n ≤  and let k, l ∈ N. We say that L satisfies condition W (n, k, l) if |Z(n, L, k)| ≥ l.We also call condition W (, k, l) the l-weak k-prefix overlapping property, condition W (, k, l) the l-weak k-suffix overlapping property, condition W (, k, l) the l-weak k-overlapping property, condition W (, k, l) the l-weak k-double-sided overlapping property, condition W (, k, l) the l-weak k-retrograde overlapping property and condition W (, k, l) the l-weak k-concatenated retrograde overlapping property.For example, a language L satisfies W (, k, l) if and only if there exists l distinct words w ∈ L whereby w = uv for some u, v, x ∈ Σ * , with u of length at least k,x non-empty and xu ∈ L.We also define a strong version of the properties above.We also refer to each of these properties by replacing the prefix \"l-weak\" of each condition above with \"strong\".  We now consider the relationships of these properties to each other. The following is immediate from the definitions.Then the following are true:Also, we note the following:Then the following are true:Proof. The first three statements are straightforward. For the fourth statement,Combining Lemma , , we obtain:Then the following statements are true:We see, however that if L  = {abc, aa} andWe also define the following sets which we will use for a characterization.Hence, there exists x ∈ Σ + such that xu ∈ L. Let i = . \"⊆\" Let w ∈ U (, L, k). Thus, there exists v ∈ Σ ≥k , u ∈ Σ * , y ∈ Σ + , vy ∈ L, w = uv. Therefore, v ∈< − p (L) ∩ Σ ≥k and w ∈≤ s (< − p (L) ∩ Σ ≥k ). \"⊇\" Let w ∈≤ s (< − p (L) ∩ Σ ≥k ). Thus, there exists u, v ∈ Σ * such that w = uv with v ∈< − p (L) ∩ Σ ≥k . Hence, there exists x ∈ Σ + such that vx ∈ L.Let i = . Immediate from case , . Let i = . \"⊆\" Let w ∈ U (, L, k). Thus, there exists u, v ∈ Σ ≥k , x, y ∈ Σ + , w = uv, (xu ∈ L∧vy ∈ L). Therefore, u ∈< −Hence, there exists x, y ∈ Σ + such that xu ∈ L and vy ∈ L.Let i = . \"⊆\" Let w ∈ U (, L, k) .This leads naturally to some decision problems. One would like to provide algorithms to test whether languages (or genomes) satisfy these properties. Namely, can we decide whether a given language satisfies one of the properties, depending on the language family that the given language is in? For each weak condition, this amounts to deciding whether |Z(i, L, k)| ≥ l and for each strong condition, it amounts to deciding whether Z(i, L, k) = L. Proposition . Let L  , L  be language families effectively closed under intersection and the full trio operations with L  being effectively semilinear and L  having a decidable equality problem. Then the following are true:. For each k, l ∈ N and i,  ≤ i ≤ , it is decidable whether L ∈ L  satisfies W (i, k, l) and it is decidable whether L ∈ L  satisfies V (i, k). . If L  , L  are also effectively closed under reversal, then it is decidable whether L ∈ L  satisfies W (, k, l) and it is decidable whether L ∈ L  satisfies V (, k). . If L  , L  are also effectively closed under reversal and +, then it is decidable whether L ∈ L  satisfies W (, k, l) and it is decidable whether L ∈ L  satisfies V (, k).Also, every intersectionclosed full trio is closed under union and concatenation since L  $Σ * ∩ Σ * $L  is in L  and L  , there is an a-transducer which outputs L  ∪ L  and there is a homomorphism which outputs L  L  . Thus, Z(, L, k), Z(, L, k), Z(, L, k), Z(, L, k) are in L  and L  . Additionally, if L  , L  are closed under reversal, then Z(, L, k) is in L  and L  and if L  , L  are closed under reversal and +, then Z(, L, k) is in L  and L  . Since L  is effectively semilinear, we can decide if L ∈ L  is infinite [] and if it is not, then we can effectively find the length of the longest word in L. Then, we can test membership of every word of length less than or equal to that length to determine whether |Z(i, L, k)| ≥ l (emptiness is always decidable for semilinear sets, and since L  is closed under intersection with regular languages, we can decide whether w ∈ L by testing whether L∩{w} = ∅). Also, by the decidability of equality for L  , the proposition follows.We denote by NCM the family of languages defined by one-way nondeterministic, reversal-bounded multicounter machines. It is known that NCM is an intersection and reversal closed full trio effectively closed under semilinearity [] . Also, it is known that the family of regular languages is closed under all of the operations above and has a decidable equality problem.For each L ∈ NCM, each i,  ≤ i ≤  and each k, l ∈ N, it is decidable whether L satisfies W (i, k, l). In addition, for each L ∈ REG, each i,  ≤ i ≤  and each k, l ∈ N, it is decidable whether L satisfies W (i, k, l) and V (i, k).Ideally, one would like to apply the formal definitions given here to real viral genomes as a method for classifying viruses based on gene compression. In this section we will consider fast algorithms to do exactly this, and their complexity. Since all real viral genomes are finite, we will restrict ourselves to dealing with finite input languages here. We will describe algorithms which will verify each of the viral properties for a given input viral genome. A viral genome is a finite language in which the words are the genes of the virus.For a finite language L ⊆ Σ + , we let s L be the sum of the lengths of every word of L (the length of the genome).We recall a well-known and important result from [] . A partial deterministic finite automaton is a deterministic finite automaton in which each state need not have a transition on every letter. The smallest partial DFA for a given regular language is the partial DFA that recognizes the language and has the smallest number of states. In [] , it is demonstrated that, for each word w ∈ Σ * , the smallest partial DFA accepting For each algorithm in this section, we assume that we have some encoding of L as input, whereby there is only one copy of each word given. We have discussed above how to perform the method suffix dfa. It is easy to pass in the reversal of a language to suffix dfa, in time linear in s L . Then, in line  of Algorithm , we can check to see if the intersection is empty by keeping a counter starting at k and running w through the transition function of M , decreasing the counter at each step. Then, when the counter reaches , we test every state we hit on input w to see whether it is a final state. If it is, we increase l  and set v  indicating that w ∈ Z(, L, k). Also, in line , we are testing whether w R ∈ Z(, L R , k). Indeed, by Lemma (), w R ∈ Z(, L R , k) if and only if w ∈ Z(, L, k). Thus, if this is true, we increase l  and set v  to true. In addition, w ∈ Z (, L, k) if and only if w ∈ Z(, L, k) ∪ Z(, L, k) and so we increase l  if and only if either v  or v  is true, and we reset each to false. In this way, when the method completes, l  , l  and l  will be the maximum such that L satisfies W (, k, l  ), W (, k, l  ) and W (, k, l  ), respectively. Furthermore, this method runs in time O(s L ) time. In addition, it is well-known that we can test whether a word w is in the language generated by an NFA in time O(|Q||w|) (see [] ). Thus, to find the largest integer l  such that L satisfies W (, k, l), we construct the NFA from L and decide membership of w R for each w ∈ L. This takes time O(|w  ||Q| + · · · + |w m ||Q|) = O(|Q|s L ). Thus, one can decide whether a finite language L satisfies W (, k, l) in time O(s  L ). Finally, the strong properties can also be verified straightforwardly using the algorithms presented above. Indeed, they are just a special case where l = |L|. We summarize the preceding thusly: Proposition . Let i satisfy  ≤ i ≤  and let Σ be some fixed alphabet. Then given a finite language L ⊆ Σ + as input without duplicates and k ∈ N, we can both find the largest integer l such that L satisfies W (i, k, l) and we can decide whether L satisfies V (i, k) in time O(s L ). Furthermore, we can both find the largest l whereby L satisfies W (, k, l) and we can decide whether L satisfies V (, k) in time O(s  L ).We have presented here a formalization of the process of gene compression that occurs in many viral genomes. We have shown dependencies and relationships between these properties and demonstrated that, in general, most of the weak versions of the properties can be decided for languages defined by nondeterministic finite automata augmented with reversal-bounded counters while the strong versions can be decided for regular languages. Most significantly, we have given algorithms which can efficiently decide these properties for real viral genomes and provide information which is immediately useful to virologists. These algorithms give us the ability to study the relative amount of gene compression between related viruses in a quantifiable way. It may be possible to infer evolutionary relationships between viruses using this information. The fact that genes overlap one another provides a very serious constraint for viral genome evolution. It is known that viruses occasionally aquire genes horizontally (that is, a gene from an infected host becomes part of the virus\\'s own genome). Clearly, only those genes which meet very specific constraints (e.g. those that are \"compressible\" relative to the virus\\'s genome) will be able to be incorporated into the virus. Using the algorithms presented here and real viral genome data, we can find target genes in the host organism which, due to their structure, have the greatest probability of being incorporated into the viral genome.Finally, the formal properties here also present a framework for automated classification of a virus given only its genome. The family of Coronaviruses, for example, has a very regular genomic structure: a single strand of +-sense RNA of length -kb. The beginning of this RNA strand always encodes a viral polymerase (often as part of a polyprotein) and the remainder encodes a series of \"nested\" genes. Each of these nested genes is a proper suffix of the previous gene.This structure can obviously be formally encoded using the properties given here. Similar compression regularities can be found in other viral genomes and encoded using our properties. Classification of a new virus is then simply a matter of verifying compliance to our properties and then checking to see if this matches any known structures.By formalizing this ancient form of data compression, we have provided tools which will allow for further insight in the molecular evolution of viruses and assist in the automated classification of new viruses by reference to only their genomes. '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove numbers\n",
    "import re\n",
    "pp_text = re.sub(r'\\d+', '', sample_txt)\n",
    "\n",
    "pp_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d7d107-dcdc-44e6-952b-088ee9842cbc",
   "metadata": {},
   "source": [
    "____________\n",
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25129fca-37dd-4933-9b53-aeb6e934b130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#tokenize\n",
    "tokens = nltk.word_tokenize(pp_text)\n",
    "#tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798ecae7-5954-4c6a-8ca4-b2be8ce51156",
   "metadata": {},
   "source": [
    "__________________________________________________\n",
    "## Lemmatization\n",
    "### Uses context to convert word into a meaningful form :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39ab2138-be3d-4d77-a79f-4e11c1b1a8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Hayagreev\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Hayagreev\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('organisms', 'organism'),\n",
       " ('viruses', 'virus'),\n",
       " ('adaptations', 'adaptation'),\n",
       " ('genes', 'gene'),\n",
       " ('genomes', 'genome'),\n",
       " ('constraints', 'constraint'),\n",
       " ('strategies', 'strategy'),\n",
       " ('properties', 'property'),\n",
       " ('as', 'a'),\n",
       " ('languages', 'language'),\n",
       " ('applications', 'application'),\n",
       " ('possess', 'posse'),\n",
       " ('introns', 'intron'),\n",
       " ('systems', 'system'),\n",
       " ('types', 'type')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4') # open multi-lingual wordnet\n",
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "# Function to lemmatize the corpus and return lemmatized words (original and the lemmatized)\n",
    "def lemmatizer(s):\n",
    "\n",
    "    \n",
    "    #Initialize the lemmatizer\n",
    "    lemmatize = WordNetLemmatizer()\n",
    "    \n",
    "    #store the stemmed words in the dictionary\n",
    "    lem_tokens = {}\n",
    "    \n",
    "    # Lemmatize all tokens\n",
    "    for token in tokens:\n",
    "        lem_token = lemmatize.lemmatize(token)\n",
    "        if lem_token != token:\n",
    "            lem_tokens[token] = lem_token\n",
    "    return lem_tokens\n",
    "\n",
    "lemmatized_tokens = lemmatizer(sample_txt)\n",
    "\n",
    "# List N lemmatized tokens\n",
    "list(lemmatized_tokens.items())[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d72d35c-ee65-44aa-9f20-c6ff36ee1478",
   "metadata": {},
   "source": [
    "__________________________________________________\n",
    "## Stemming\n",
    "### Algorithm to Remove suffixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1edfeb9f-ec59-4b5b-be37-5f0cf0200223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('material', 'materi'),\n",
       " ('inside', 'insid'),\n",
       " ('serves', 'serv'),\n",
       " ('protection', 'protect'),\n",
       " ('mechanism', 'mechan'),\n",
       " ('inserting', 'insert'),\n",
       " ('cases', 'case'),\n",
       " ('mixture', 'mixtur'),\n",
       " ('possibly', 'possibl'),\n",
       " ('proteins.The', 'proteins.th'),\n",
       " ('stems', 'stem'),\n",
       " ('limits', 'limit'),\n",
       " ('Polyomaviruses', 'polyomavirus'),\n",
       " ('constrained', 'constrain'),\n",
       " ('approximately', 'approxim')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def stemmer(s):\n",
    "\n",
    "    #initialize the Porter Stemmer\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    #store all the stemmed tokens in this dictionary\n",
    "    stemmed_tokens = {}\n",
    "    \n",
    "    # stem all tokens\n",
    "    for token in tokens:\n",
    "        stemmed_token = ps.stem(token)\n",
    "        if stemmed_token != token:\n",
    "            stemmed_tokens[token] = stemmed_token\n",
    "    return stemmed_tokens\n",
    "\n",
    "stem_tokens = stemmer(tokens)\n",
    "\n",
    "list(stem_tokens.items())[100:115]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138cf1ce-a8cb-4641-9c44-3fbbaa900dba",
   "metadata": {},
   "source": [
    "______________________________\n",
    "_________________________\n",
    "# Day 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462a8aa1-66c5-43ac-9f5a-6ab7ba64724f",
   "metadata": {},
   "source": [
    "______________________\n",
    "##Vector space\n",
    "### One Hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "672f179b-d847-4267-bff0-39ff198f61f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "703\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'provides': array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'quantifiable': array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'question': array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'rare': array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]),\n",
       " 'reaches': array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]),\n",
       " 'read': array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]),\n",
       " 'reader': array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " 'real': array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]),\n",
       " 'recall': array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " 'recognizes': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-hot encoding\n",
    "import numpy as np\n",
    "vocab = sorted(list(set(tokens)))\n",
    "print(len(vocab))\n",
    "small_vocab = vocab[-200:-190]\n",
    "\n",
    "#generate one-hot encoding\n",
    "ohv_dict = {}\n",
    "for i,token in enumerate(small_vocab):\n",
    "    ohv_temp = np.zeros(len(small_vocab))\n",
    "    ohv_temp[i] = 1\n",
    "    ohv_dict[token] = ohv_temp\n",
    "ohv_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35db3033-c60f-4442-bb1c-6e9f4210c374",
   "metadata": {},
   "source": [
    "_________________________\n",
    "##Cosine similarity\n",
    "## Cosine similarity betweeen any two vectors $u$ and $v$ is defined as\n",
    " $$ \\cos(\\theta) = \\dfrac{u.v}{{||u^2||\\,||v^2||}}$$\n",
    " \n",
    " ### $\\cos(\\theta)$ decreases as the similarity between $u$ and  $u$ decreases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ba07fa-2ebb-42f2-9e24-bc3bdd421510",
   "metadata": {},
   "source": [
    "_______________\n",
    "## Cosine Distance\n",
    "## Cosine distance betweeen any two vectors $u$ and $v$ is defined as\n",
    "$$ CD = 1 -  \\dfrac{u.v}{{||u^2||\\,||v^2||}}$$\n",
    " \n",
    "### CD increases as the similarity between $u$ and  $u$ decreases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b940b1e-b3eb-47ff-961e-8e640d6cdef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20210448218137755\n",
      "0.42264973081037416\n",
      "0.21337571476754802\n",
      "0.20210448218137755\n",
      "0.0016510660689108558\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "###### \n",
    "from scipy.spatial import distance\n",
    "color1 = [255,255,255] # white\n",
    "color2 = [255,0,0] # red\n",
    "color3 = [255,144,0] # orange\n",
    "color4 = [255,164,0] # lighter shade of orange\n",
    "color5 = [0,0,255] # Blue\n",
    "color6 = [0,255,0] # Green\n",
    "\n",
    "print(distance.cosine(color1,color4)) # between white and lighter shade of orange\n",
    "print(distance.cosine(color1,color2)) # between white and red\n",
    "print(distance.cosine(color1,color3)) # between white and orange\n",
    "print(distance.cosine(color1,color4)) # between white and lighter shade of orange\n",
    "\n",
    "print(distance.cosine(color3,color4)) # between orange and lighter shade of orange\n",
    "print(distance.cosine(color2,color5)) # between red and blue\n",
    "print(distance.cosine(color2,color6)) # between red and green"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67754088-e95e-4593-a2f9-e97d8d0a667b",
   "metadata": {},
   "source": [
    "______________________\n",
    "# Contextual represetation using bigrams, trigrams, etc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1978aceb-6bc1-4161-bcee-1c940638a828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 'staff')\n",
      "('staff', 'has')\n",
      "('has', 'undergone')\n",
      "('undergone', 'on-line')\n",
      "('on-line', '36-hour')\n",
      "('36-hour', 'training')\n",
      "('training', 'course')\n",
      "('course', 'to')\n",
      "('to', 'become')\n",
      "('become', 'qualified')\n",
      "('qualified', 'in')\n",
      "('in', 'coronavirus')\n",
      "('coronavirus', 'infection')\n",
      "('infection', 'treatment')\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "txt = \"the staff has undergone on-line 36-hour training course to become qualified in coronavirus infection treatment\"\n",
    "bigrams = nltk.ngrams(nltk.word_tokenize(txt),2) \n",
    "for bigram in bigrams: \n",
    "    print(bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1929262-9023-4696-bff3-f5ac8df643a6",
   "metadata": {},
   "source": [
    "_____________\n",
    "## Trigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b15364f4-ccb8-49e5-a1ad-26f6ee308c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 'staff', 'has')\n",
      "('staff', 'has', 'undergone')\n",
      "('has', 'undergone', 'on-line')\n",
      "('undergone', 'on-line', '36-hour')\n",
      "('on-line', '36-hour', 'training')\n",
      "('36-hour', 'training', 'course')\n",
      "('training', 'course', 'to')\n",
      "('course', 'to', 'become')\n",
      "('to', 'become', 'qualified')\n",
      "('become', 'qualified', 'in')\n",
      "('qualified', 'in', 'coronavirus')\n",
      "('in', 'coronavirus', 'infection')\n",
      "('coronavirus', 'infection', 'treatment')\n"
     ]
    }
   ],
   "source": [
    "trigrams = nltk.ngrams(nltk.word_tokenize(\"the staff has undergone on-line 36-hour training course to become qualified in coronavirus infection treatment\"),3)\n",
    "for trigram in trigrams:\n",
    "    print(trigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c3439b-a212-4382-86ff-79901b4c1333",
   "metadata": {},
   "source": [
    "_____________\n",
    "# Exercise for Day 2\n",
    "## 1. What is length of the OneHot Vector if the vocabulary size was 10000\n",
    "## 2. Consider any small corpus having ~10 sentences. Write a Python program to create a co-occurence matrix\n",
    "## 3. Take the same corpus as above and create a co-occurence matrix using bigrams after removing stop words. \n",
    "##    Make sure that you have only English words in the pre-processed corpus. Find the vocabulary of the pre-processed \n",
    "##    corpus. Unigrams represent the vocabuly. \n",
    "##    Create a co-occurence matrix using bigrams columns and unigrams as rows. If a unigram was present in a bigram, then \n",
    "##    add 1 to the existing value (if any)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41b8bd3-f194-4001-b685-4b1220f3f200",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "f637829467794aff9fd82023e4db8aff997e8bf1c51aa5b4002f786773a0252b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
